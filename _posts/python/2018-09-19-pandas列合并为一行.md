---
layout: post
categories: [python]
title: pandas列合并为一行
date: 2018-09-19
author: TTyb
desc: "将dataframe利用pandas列合并为一行，类似于sql的GROUP_CONCAT函数"
---

将`dataframe`利用`pandas`列合并为一行，类似于`sql`的`GROUP_CONCAT`函数。例如如下`dataframe`

~~~ruby
  id_part         pred   pred_class v_id
0       d  0 0.122817         woman   d1
1       b     0.015449  other_label   d2
2       5     0.019208          cat   d3
3       d     0.050064          dog   d1
~~~

想要变成如下形式：

~~~ruby
  v_id   pred_class                     pred id_part
0   d1   woman, dog  [0 0.122817 , 0.050064]       d
1   d2  other_label               [0.015449]       b
2   d3          cat               [0.019208]       5
~~~

利用 `groupby` 去实现就好，`spark`里面可以用 `concat_ws` 实现，可以看这个 [Spark中SQL列合并为一行](http://www.tybai.com/scala/Spark%E4%B8%ADSQL%E5%88%97%E5%90%88%E5%B9%B6%E4%B8%BA%E4%B8%80%E8%A1%8C.html)，而这里没有 `concat_ws` 只能用另外一种方式实现：

~~~ruby
df2 = other_label.groupby(['v_id']).agg({'pred_class': [', '.join],
                                         'pred': lambda x: list(x),
                                         'id_part': 'first'}).reset_index()
~~~


得到结果为：

~~~ruby
  v_id   pred_class                     pred id_part
0   d1   woman, dog  [0 0.122817 , 0.050064]       d
1   d2  other_label               [0.015449]       b
2   d3          cat               [0.019208]       5
~~~

而还有另外一种方式，但是可能会输出少了那么几列：

~~~ruby
df1 = data.groupby(['v_id', 'id_part'])['pred_class'].apply(lambda x: list(x)).reset_index()
~~~

~~~ruby
  v_id id_part     pred_class
0   d1       d   [woman, dog]
1   d2       b  [other_label]
2   d3       5          [cat]
~~~